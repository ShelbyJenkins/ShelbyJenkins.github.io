---
title: 'Cascade Prompt'
# Optional description: If provided the description field will be used for previews. If not provided, the first 3-4 lines from the post will be used as a description.
description: 'This post is used to generate new posts. It also contains instructions for adding posts. '

# Published date is required and in the format of ISO-8601: `yyyy-mm-dd`. For more info see https://docs.astro.build/en/guides/content-collections/#working-with-dates-in-the-frontmatter
pubDate: 2024-08-31
# Optionally specify an update date. If not provided, one will be generated from the git history. Only if the post has been changed since the day published.
modifiedDate: 2024-08-31
heroImage: './hero.png'
# Optional
heroAlt: 'Describe the image for screen readers.'
---

# The Prompt Engineering Dichotomy

1. Prompt engineering is a legitimate and required skill for implementing LLMs.
2. Prompt engineering is a meme, and if you put "prompt engineer" in your LinkedIn profile, _you_ are the meme.

![The proompter shirt.](./proompter.jpg)(class:'small')

<p class="caption">But you can wear this shirt ironically.</p3>

Sorry to put you in a bucket, but that's life when playing with a technology at the peak of its hype cycle. "All you need to master AI is prompting, and I've already mastered English ‚Äî I speak it every day!" Apologies to all the marketing copywriters out there who are probably reliving every bad experience with managers and colleagues who have emphatically _not_ mastered the English language, devaluing their skills and passion as writers.

![The proompter shirt.](./english-majors-like.jpg)(class:'medium')

If you use LLMs as a consumer, you should at least have a working knowledge of prompting. **If you implement LLMs, you really should have a deep understanding of the weird nexus of natural language, database queries, and ML inference that is prompting.**

# Deterministic Signal from Probabilistic LLM Vibes

Vicki Boykis wrote this great blog post, ["We've been put in the vibe space,"](https://vickiboykis.com/2024/05/06/weve-been-put-in-the-vibe-space/) that's been stuck in my head for most of 2024.

> When we go to Twitter/Bluesky/Instagram/Discord/OpenTwang and we are served channel or post or image recommendations, we recognize that we are in the vibe space and can only influence these through our explicit and implicit behavior on a given site, which goes to some great matrix factorization in the sky and pulls out recommended items into a stream, or a timeline, or a carousel. We do this all publicly with the expectation that all our data is harvested and reintegrated into The Algorithm.

![Vicki Boykis's tweet.](./flutter.png)(class:'small')

<p class="caption">A critique of a tech stack, in the format of said tech stack; vibes over facts.</p3>

Essentially, it boils down to the old world of deterministic computer systems (if statements and SQL queries) and the new world of vibe stacks (LLMs and semantic search). Unlike the deterministic outputs of if statements and database queries, raw LLMs produce outputs that can be described as "vibes" ‚Äî probabilistic, context-dependent results that may vary between runs. Building on vibes may work for creative applications, but in real-world applications, a lossy, vibey output is a step back from the reliability and consistency of traditional programming paradigms.

And prompting can help with this, but 'prompting' as we know it is incomplete. A science in infancy. A tech stack without best practices. A DB without an ORM (JK, I know _real_ bosses raw dog those SQL statements.)

# Traditional LLM Constraints Impact LLM Performance

Using an LLM as part of your business logic requires extracting usable signals from LLM responses. This means interpreting the text generated by an LLM using something like Regex or LLM constraints. Controlling the output of an LLM is commonly achieved with constraints like logit bias, stop words, or grammars.

However, from practical experience, as well [as studies](https://arxiv.org/abs/2408.02442), constraints such as logit bias and grammars negatively impact the quality of LLMs. If you consider LLM constraint libraries like [Microsoft's Guidance](https://github.com/guidance-ai/guidance) and [Outlines](https://github.com/outlines-dev/outlines) the query language (or some other portion of the traditional DB model), llm_client goes one step further. In addition to employing the underlying techniques of the aforementioned constraint libraries, llm_client introduces a technique called "cascading prompt workflows." This enables LLMs to be used as efficient and accurate decision-makers within traditional business logic.

# Controlled Generation with Step Based Cascade Workflows

Traditional LLM constraints work at the token level, but as good Proompters, we need to shape the structure of the entire generation process. The Cascade prompting technique runs pre-defined workflows that control and constrain both the overall structure of generation and individual tokens during inference. This allows the implementation of specialized workflows for specific tasks, shaping LLM outputs towards intended, reproducible outcomes.

This method significantly improves the reliability of LLM use cases. For example, [there are test cases this repo](./tests/common/test_sets) that can be used to benchmark an LLM. There is a large increase in accuracy when comparing [basic inference with a constrained outcome](./tests/basic_primitive_tests.rs) and [a CoT style cascading prompt workflow](./src/workflows/reason/one_round.rs). The [decision workflow](./examples/decision.rs) that runs N count of CoT workflows across a tempature gradient approaches 100% accuracy for the test cases.

Why the name 'Cascade?' IDK, it sounded cool.

# An Example Cascade: CoT Reasoning

```rust
// Load Local LLMs
let llm_client = LlmClient::llama_cpp().available_vram(48).mistral7b_instruct_v0_3().init().await?;

// Build requests
let response: u32 = llm_client.reason().integer()
    .instructions()
    .set_content("Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?")
    .return_primitive().await?;

// Recieve 'primitive' outputs
assert_eq!(response, 1)
```

If you install llm_client, and run this very üêç friendly üêç looking Rust code, you'll receive the following un-annotated output in the terminal.

![Annotated terminal output from reason one round cascade prompt workflow.](./reason_one_round_example_1.png)(class:'large')

<p class="caption">Annotated terminal output from reason one round cascade prompt workflow</p3>

You have the correct answer, you have the correct answer in a format your existing code base can use (A u32 integer), and you have a clear, repeatable process that gets you to the answer.

# This is Prompt Engineering

Some notes.

- Examples are dead; long live functional examples. Notice how the initial user message defines the format, and then the pre-generated assistant message follows it? LLMs follow patterns. _Asking_ it to implement a pattern is noisy. Show don't tell.
- Generation prefixes are so much better than instructions. Again, it's the differences between asking it to use CoT and _forcing_ it to reason. Before I implemented this, the LLM would often answer the question immediately or in the first sentence. What we _want_ is for the LLM to 'think' by regurgitating token predictions until it's at a natural place to answer the question.
- Stop words can be used creatively. In the backend, I use stop
- Build on low quality or small models. If it works on Microsoft's Phi Mini, it will work better on larger models. It's also faster on smaller models.

### Cascade Prompt Elements

- **Rounds**: Each round is a pair of a user turn and an assistant turn. Turns are sometimes referred to as 'messages'.
  - Both the user turn and the assistant turn can be pre-generated, or dynamically generated.
- **Tasks**: The 'user message' in the user turn of a round. Generically refered to 'task' for the sake of brevity.
- **Steps**: Each assistant turn consists of multiple steps.
  - **Inference steps** generate text via LLM inference.
  - **Guidance steps** generate text from pre-defined static inputs or dynamic inputs from the program.
- **Generation prefixes**: Assistant steps can be prefixed with content prior to generation.
- **Dynamic sufixes**: Assistant steps can also be suffixed with additional content after generation. (Not used here.)

Some of these things are based on common techniques. Some of them are implementations of newer, uncommon techniques. At the end of the day, it's not revolutionary, it's just text manipulation. But I guess, so was Gutenberg's printing press.

### What the _actual_ prompt looks like

```text
<|user|>
A request will be provided. Think out loud about the request. State the arguments before arriving at a conclusion with, 'Therefore, we can conclude:...', and finish with a solution by saying, 'Thus, the solution...'. With no yapping.<|end|>
<|assistant|>
'no yapping' refers to a design principle or behavior where the AI model provides direct, concise responses without unnecessary verbosity or filler content. Therefore, we can conclude: The user would like to get straight to the point. Thus, the solution is to to resolve the request as efficiently as possible.<|end|>
<|user|>
The user's request is: Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?<|end|>
<|assistant|>
Thinking out loud about the users request... The information presented here involves family relations, which suggests a mathematical approach to find a common ground. In this case, we need to understand the sibling relationships to determine the number of sisters Sally has. The user requested a conclusion of a number between 0-9999. Therefore, we can conclude: Let's assume Sally is one of the siblings, which means she counts as one sister. The question states that each brother has 2 sisters. If Sally is one of those sisters, then the other brother also has 2 sisters. This implies there is only one additional sister. The user's original request was 'Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?'. Thus, the number solution to the user's request is: 1<|end|>
<|endoftext|>
```

# Meet the Proompter

llm_client is a Rust project I've been working on for awhile. Why Rust? Because Rust has less people building on it and less packages in it's package manager, so I could claim [a great name for my project!](https://crates.io/crates/llm_client) This surely is the number one reason to choose Rust for a ML project.

What is it? Well the [README](https://github.com/ShelbyJenkins/llm_client) says:

> llm_client is an interface for building cascading prompt workflows from dynamic and novel inputs, running the steps of the workflows in a linear or reactive manner, and constraining and interpreting LLM inference as actionable signals. This allows the integration of LLMs into traditional software systems with the level of consistency and reliability that users expect.

Big, dumb words aside, what the project really does is... well, it kinda is like an ORM for LLMs I guess. The goal is to make it installable via Cargo, and fully in-process and embeddable like SQLite. Once the migration from [llama.cpp](https://github.com/ggerganov/llama.cpp) to [mistral.rs](https://github.com/EricLBuehler/mistral.rs) is complete, you'll be able to `cargo install llm_client` and be able to **integrate deterministic decision making from an LLM into any project**. Fully in Rust. No more C++ bindings and running a llama.cpp in server mode to communicate via HTTP like [Ollama](https://github.com/ollama/ollama).

Deterministic decision making from an LLM?
